{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from KoreanDataCleaning import KoreanDataCleaning\n",
    "from KoreanTokenization import KoreanTokenization\n",
    "from KoreanStopwordsRemoval import KoreanStopwordsRemoval\n",
    "from KoreanLemmatization import KoreanLemmatization\n",
    "from KoreanEncoding import KoreanEncoding\n",
    "from KoreanNormalization import KoreanNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KoreanDataCleaning => 이것은 한국어 테스트 문장 입니다.\n",
      "KoreanTokenization => ['이', '것', '은', '한국어', '테스트', '문장', '입니다', '.']\n",
      "KoreanStopwordsRemoval => ['것', '한국어', '테스트', '문장', '입니다', '.']\n",
      "KoreanLemmatization => ['것', '한국어', '테스트', '문장', '입니다', '.']\n",
      "KoreanEncoding => [3, 6, 5, 4, 2, 7]\n",
      " tensor_sequences is [tensor(3), tensor(6), tensor(5), tensor(4), tensor(2), tensor(7)]\n",
      " shape of tensor 0 is torch.Size([])\n",
      " shape of tensor 1 is torch.Size([])\n",
      " shape of tensor 2 is torch.Size([])\n",
      " shape of tensor 3 is torch.Size([])\n",
      " shape of tensor 4 is torch.Size([])\n",
      " shape of tensor 5 is torch.Size([])\n",
      " padded_data is tensor([[3],\n",
      "        [6],\n",
      "        [5],\n",
      "        [4],\n",
      "        [2],\n",
      "        [7]])\n",
      "KoreanNormalization => tensor([[3],\n",
      "        [6],\n",
      "        [5],\n",
      "        [4],\n",
      "        [2],\n",
      "        [7]])\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    korean_text = \"이것은 한국어 테스트 문장 입니다.\"\n",
    "    \n",
    "    # step 1 datacleaning \n",
    "    cleaner = KoreanDataCleaning(korean_text)\n",
    "    cleaner.remove_special_characters()\n",
    "    cleaner.remove_html_tags()\n",
    "    cleaner.to_lowercase()\n",
    "    cleanned_text = cleaner.get_text()\n",
    "    print(f\"KoreanDataCleaning => {cleanned_text}\")\n",
    "    \n",
    "    # step 2 tokenization\n",
    "    tokenizer = KoreanTokenization(cleanned_text)\n",
    "    korean_tokens = tokenizer.tokenize()\n",
    "    print(f\"KoreanTokenization => {korean_tokens}\")\n",
    "    \n",
    "    #step 3 stopwordsRemoval\n",
    "    stopwords_removal = KoreanStopwordsRemoval(korean_tokens)\n",
    "    filterd_korean_tokens = stopwords_removal.remove_stopwords()\n",
    "    print(f\"KoreanStopwordsRemoval => {filterd_korean_tokens}\")\n",
    "\n",
    "    #step 4 lemmatization\n",
    "    lemmatizer= KoreanLemmatization(filterd_korean_tokens)\n",
    "    lemmatized_korean_tokens = lemmatizer.apply_lemmatization()\n",
    "    print(f\"KoreanLemmatization => {lemmatized_korean_tokens}\")\n",
    "    \n",
    "    #step 5 encoding\n",
    "    encoder = KoreanEncoding(lemmatized_korean_tokens)\n",
    "    encoded_korean_tokens = encoder.encode()\n",
    "    print(f\"KoreanEncoding => {encoded_korean_tokens}\")\n",
    "\n",
    "    #step 6 normalization\n",
    "    normalizer = KoreanNormalization(encoded_korean_tokens)\n",
    "    normalized_data = normalizer.normalize(max_length=10)\n",
    "    print(f\"KoreanNormalization => {normalized_data}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vir-env.jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
